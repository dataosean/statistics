{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24.08.23 통계 학습 13회차: 최대우도추정법(MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💡최대우도추정법(MLE) 요약\n",
    "\n",
    "1. 최대우도추정법(MLE)은 주어진 데이터에 대한 확률 모델의 매개변수를 추정하는 통계적 방법이다.\n",
    "\n",
    "2. 이 방법은 데이터가 관찰될 확률(우도)을 최대화하는 매개변수를 찾는다.\n",
    "\n",
    "3. MLE는 다양한 분포에 적용 가능하며, 큰 표본에서 점근적 일치성과 효율성을 보장한다.\n",
    "\n",
    "4. 그러나 작은 표본에서는 불편성 문제를 가질 수 있다.\n",
    "\n",
    "5. 실무에서는 회귀 분석 및 머신러닝 모델의 매개변수 추정에 널리 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔎목차\n",
    "\n",
    "1. 최대우도추정법 개요\n",
    "\n",
    "    - 최대우도추정법(MLE)이란 무엇인가?\n",
    "    - 최대우도추정법의 역사와 발전 과정\n",
    "    - MLE의 중요성과 데이터 분석에서의 역할\n",
    "\n",
    "2. 최대우도추정법의 이론적 기초\n",
    "\n",
    "    - 우도(Likelihood)의 개념\n",
    "    - 확률밀도함수(PDF)와 우도 함수의 관계\n",
    "    - 매개변수 추정의 필요성 및 MLE의 기본 원리\n",
    "\n",
    "3. 최대우도추정법의 수학적 정의\n",
    "\n",
    "    - 우도 함수(Likelihood Function) 정의\n",
    "    - 우도 함수의 로그 변환 (Log-Likelihood)\n",
    "    - 최대화 문제와 MLE 수식 유도\n",
    "\n",
    "4. 최대우도추정법의 계산\n",
    "\n",
    "    - 우도 함수의 도함수를 통한 매개변수 추정\n",
    "    - 로그 우도 함수 최대화의 필요성\n",
    "    - 수치적 최적화 기법을 이용한 MLE 계산 방법\n",
    "\n",
    "5. MLE의 구체적 사례\n",
    "\n",
    "    - 이항 분포에서의 MLE 적용\n",
    "    - 정규 분포에서의 MLE 적용\n",
    "    - 포아송 분포에서의 MLE 적용\n",
    "    - 기타 분포에서의 MLE 적용 (예: 지수 분포, 다항 분포 등)⭐\n",
    "\n",
    "6. 최대우도추정법의 성질\n",
    "\n",
    "    - 점근적 일치성 (Asymptotic Consistency)\n",
    "    - 효율성 (Efficiency)\n",
    "    - 불편성 (Unbiasedness)\n",
    "\n",
    "7. MLE의 장점과 한계\n",
    "\n",
    "    - MLE의 장점: 일반성, 수치적 접근 가능성\n",
    "    - MLE의 한계: 계산 복잡성, 극값 존재 여부\n",
    "    - 작은 표본 크기에서의 문제점\n",
    "\n",
    "8. 최대우도추정법의 확장\n",
    "\n",
    "    - 베이즈 추정과의 비교\n",
    "    - 우도비 검정 (Likelihood Ratio Test)\n",
    "    - EM 알고리즘을 통한 MLE 확장\n",
    "    - 다변량 MLE 기법 (예: 다변량 정규 분포 등)⭐\n",
    "\n",
    "9. MLE의 실무적 적용\n",
    "\n",
    "    - MLE를 활용한 회귀 분석\n",
    "    - 머신러닝 모델의 파라미터 추정에서의 활용\n",
    "    - 실제 사례 연구 및 응용 분야\n",
    "    - MLE의 소프트웨어 구현 (Python을 통한 구현)⭐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 최대우도추정법 개요\n",
    "\n",
    "1) 최대우도추정법(MLE, Maximum Likelihood Estimation)이란 무엇인가?\n",
    "\n",
    "    - 주어진 데이터 집합을 기반으로 통계 모델의 매개변수(모수, parameter)를 추정하는 방법이다.\n",
    "    \n",
    "    - MLE는 데이터가 관찰될 확률을 최대화하는 매개변수를 찾는 것을 목표로 한다.\n",
    "\n",
    "2) 최대우도추정법의 역사와 발전 과정\n",
    "\n",
    "    - MLE는 20세기 초 영국의 통계학자 로널드 피셔(Ronald A. Fisher)에 의해 개발되었다.\n",
    "    \n",
    "    - 그는 1922년에 MLE의 개념을 정립하고 이를 통계적 추정 이론에 통합했다.\n",
    "    \n",
    "    - 이후 MLE는 다양한 통계 모델과 머신러닝 알고리즘에서 널리 사용되며, 데이터 분석의 중요한 도구로 자리 잡았다.\n",
    "\n",
    "3) MLE의 중요성과 데이터 분석에서의 역할\n",
    "\n",
    "    - MLE는 통계 모델의 모수를 추정하는 데 널리 사용되며, 회귀 분석, 분류 문제, 시계열 분석 등 다양한 데이터 분석 기법에서 필수적이다.\n",
    "    \n",
    "    - 특히, 머신러닝 모델의 학습 과정에서 MLE는 손실 함수를 최적화하는 데 사용된다.\n",
    "\n",
    "    - MLE는 비모수적 방법과 비교하여 더 강력한 성능을 보여주는 경우가 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 최대우도추정법의 이론적 기초\n",
    "\n",
    "1) 우도(Likelihood)의 개념\n",
    "\n",
    "    - 우도는 주어진 데이터가 특정 매개변수 하에서 관찰될 확률이다.\n",
    "\n",
    "    - 우도는 확률과 비슷하지만, 확률은 결과에 대한 함수인 반면, 우도는 매개변수에 대한 함수이다.\n",
    "\n",
    "    - 수학적으로, 우도는 다음과 같이 정의된다.\n",
    "\n",
    "        - $L(\\theta) = P(X | \\theta)$\n",
    "\n",
    "        - 여기서 $L(\\theta)$는 매개변수 $\\theta$에 대한 우도 함수이며, $X$는 관찰된 데이터이다.\n",
    "    \n",
    "        - 즉, 우도는 데이터가 주어진 매개변수 $\\theta$ 하에서 발생할 확률을 나타낸다.\n",
    "\n",
    "2) 확률밀도함수(PDF)와 우도 함수의 관계\n",
    "\n",
    "    - 확률밀도함수(PDF)는 연속형 확률변수의 경우, 특정 값이 발생할 확률의 밀도를 나타낸다.\n",
    "    \n",
    "    - MLE에서는 우도 함수가 PDF와 밀접한 관계를 가진다.\n",
    "\n",
    "        - 우도 함수는 주어진 데이터가 특정 매개변수 하에서 발생할 확률의 곱으로 정의된다.\n",
    "    \n",
    "    - 특히, 연속형 데이터의 경우 우도 함수는 다음과 같이 표현된다.\n",
    "\n",
    "        - $L(\\theta) = \\prod_{i=1}^{n} f(x_i | \\theta)$\n",
    "\n",
    "        - 여기서 $f(x_i | \\theta)$는 매개변수 $\\theta$ 하에서 데이터 $x_i$의 확률밀도함수(PDF)이다.\n",
    "        \n",
    "        - 즉, 모든 관측값의 PDF를 곱하여 우도 함수를 구성한다.\n",
    "\n",
    "3) 매개변수 추정의 필요성 및 MLE의 기본 원리\n",
    "\n",
    "    - 필요성: 데이터 분석에서 매개변수 추정은 모델의 성능에 큰 영향을 미친다.\n",
    "    \n",
    "    - 기본 원리: MLE의 목적은 우도 함수를 최대화하는 매개변수를 찾는 것이다.\n",
    "\n",
    "        - 이는 관측된 데이터를 가장 잘 설명하는 매개변수를 의미한다.\n",
    "    \n",
    "    - 수식 표현: 수학적으로, 이는 다음과 같은 최적화 문제로 표현된다.\n",
    "\n",
    "        - $\\hat{\\theta} = \\arg \\max_{\\theta} L(\\theta)$"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 최대우도추정법의 수학적 정의\n",
    "\n",
    "1) 우도 함수(Likelihood Function) 정의\n",
    "\n",
    "    - 우도 함수는 매개변수 $\\theta$에 대한 데이터 $X$의 우도를 정의한다.\n",
    "    \n",
    "    - 이 함수는 매개변수의 값에 따라 데이터가 발생할 가능성을 나타내며, MLE의 핵심이다.\n",
    "\n",
    "    - 주어진 데이터 $𝑋=(𝑥1,𝑥2,…,𝑥𝑛)$와 모수 𝜃에 대해, 우도 함수 𝐿(𝜃)는 다음과 같이 정의된다.\n",
    "\n",
    "        - ![image.png](attachment:image.png)\n",
    "\n",
    "        - 여기서 𝑓(𝑥𝑖∣𝜃)는 모수 𝜃에 대한 확률밀도함수(PDF)이다.\n",
    "\n",
    "2) 우도 함수의 로그 변환(Log-Likelihood)\n",
    "\n",
    "    - 우도 함수를 직접 최대화하는 것보다, 로그 우도 함수(Log-Likelihood)를 최대화하는 것이 수학적으로 더 간편하다.\n",
    "\n",
    "    - 우도 함수는 곱셈 형태로 구성되므로, 계산이 복잡할 수 있기 때문이다.\n",
    "    \n",
    "    - 이를 해결하기 위해 로그 변환을 사용하며, 로그 변환은 다음과 같이 정의된다.\n",
    "\n",
    "        - $\\ell(\\theta) = \\log L(\\theta) = \\sum_{i=1}^{n} \\log f(x_i | \\theta)$\n",
    "\n",
    "    - 로그 우도 함수 $\\ell(\\theta)$는 우도 함수의 로그를 취한 것으로, 곱셈이 덧셈으로 변환되어 계산이 용이해진다.\n",
    "\n",
    "3) 최대화 문제와 MLE 수식 유도\n",
    "\n",
    "    - MLE의 목표는 로그 우도 함수 log𝐿(𝜃)를 최대화하는 매개변수 𝜃를 찾는 과정이다.\n",
    "\n",
    "        - 이를 통해 최적의 매개변수 추정치를 구할 수 있다.\n",
    "    \n",
    "    - 따라서 연구자는 다음과 같이 최적화 문제를 해결한다.\n",
    "\n",
    "        - $\\hat{\\theta} = \\arg \\max_{\\theta} \\ell(\\theta)$\n",
    "\n",
    "    - 이 문제를 해결하기 위해 `도함수`를 사용하여 로그 우도 함수의 극대값을 찾는다.\n",
    "    \n",
    "        - 즉, 로그 우도 함수의 `도함수`를 구하고 이를 0으로 설정하여 매개변수 $\\theta$의 값을 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 최대우도추정법의 계산\n",
    "\n",
    "1) 우도 함수의 도함수를 통한 매개변수 추정\n",
    "\n",
    "    - 로그 우도 함수를 미분하여 도함수가 0이 되는 지점을 찾으면, 그 지점이 MLE로 추정된 매개변수다.\n",
    "    \n",
    "    - 예를 들어, 정규 분포에서 MLE는 평균과 분산을 추정하는 데 사용된다.\n",
    "\n",
    "    - 로그 우도 함수의 도함수는 다음과 같이 정의되며, 로그 우도 함수가 최대가 되는 지점을 찾는 과정이다.\n",
    "\n",
    "        - $\\frac{d\\ell(\\theta)}{d\\theta} = 0$\n",
    "\n",
    "    - 이 방정식을 풀면 매개변수 $\\hat{\\theta}$를 얻게 된다.\n",
    "\n",
    "2) 로그 우도 함수 최대화의 필요성\n",
    "\n",
    "    - 로그 우도 함수의 최대화는 MLE의 핵심이다.\n",
    "    \n",
    "        - 로그 우도를 최대화함으로써 매개변수 추정의 신뢰성을 높이고, 계산의 효율성을 개선할 수 있다.\n",
    "    \n",
    "        - 또한, 로그 변환을 사용함으로써 수치적 안정성을 유지할 수 있다.\n",
    "\n",
    "    - 로그 우도 함수는 곱셈 대신 덧셈으로 표현되므로, 계산이 더 간단하다.\n",
    "\n",
    "        - 로그 우도 함수를 최대화하는 과정에서 수치적 최적화 기법(예: 뉴턴-랩슨 방법)이 자주 사용된다.\n",
    "\n",
    "3) 수치적 최적화 기법을 이용한 MLE 계산 방법\n",
    "\n",
    "    - 때로는 로그 우도 함수의 해를 직접 구하기 어려운 경우가 있다.\n",
    "    \n",
    "    - 이럴 때는 수치적 최적화 기법(예: 경사 하강법, 뉴턴-랩슨 방법)을 사용하여 최적의 매개변수를 찾는다.\n",
    "    \n",
    "    - 이러한 알고리즘은 초기 추정값을 기반으로 반복적으로 매개변수를 업데이트하여 최적 해에 접근한다.\n",
    "\n",
    "    - Python에서는 scipy.optimize 모듈의 minimize 함수를 사용하여 MLE를 계산할 수 있다.\n",
    "\n",
    "        - 아래  코드는 정규분포의 최대우도추정법을 사용하여 매개변수 $\\mu$와 $\\sigma$를 추정하는 예제이다.\n",
    "        \n",
    "        - neg_log_likelihood 함수는 로그 우도 함수의 음수를 계산하여 최적화를 위한 함수로 사용된다.\n",
    "        \n",
    "        - minimize 함수는 주어진 초기 추정값을 바탕으로 매개변수를 최적화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대우도추정법에 의한 mu: 5.1, sigma: 0.573488351136175\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# 예제: 정규분포의 MLE 계산\n",
    "def neg_log_likelihood(params, data):\n",
    "    mu, sigma = params[0], params[1]  # mu, sigma = params도 가능하다.\n",
    "    return -np.sum(np.log(1/(np.sqrt(2*np.pi)*sigma)) - ((data - mu)**2)/(2*sigma**2))\n",
    "\n",
    "# 데이터 예시\n",
    "data = np.array([4.2, 4.5, 4.6, 4.9, 5.1, 5.3, 5.5, 5.8, 6.0])\n",
    "\n",
    "# 초기 추정값 (mu, sigma)\n",
    "initial_params = [np.mean(data), np.std(data)]\n",
    "\n",
    "# MLE 최적화\n",
    "result = minimize(neg_log_likelihood, initial_params, args=(data,), bounds=((None, None), (1e-5, None)))\n",
    "\n",
    "# 결과 출력\n",
    "mu_mle, sigma_mle = result.x\n",
    "print(f\"최대우도추정법에 의한 mu: {mu_mle}, sigma: {sigma_mle}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. MLE의 구체적 사례\n",
    "\n",
    "1) 이항 분포에서의 MLE 적용\n",
    "\n",
    "    - 이항 분포의 경우, 성공 확률 $p$를 추정하는 데 MLE를 적용할 수 있다.\n",
    "\n",
    "        - 데이터가 주어졌을 때, 𝑝를 최적화하여 우도를 최대화한다.\n",
    "    \n",
    "    - 데이터가 $n$번의 시행에서 $k$번의 성공을 기록했다고 가정하면, 우도 함수는 다음과 같이 정의된다.\n",
    "\n",
    "        - $L(p) = \\binom{n}{k} p^k (1-p)^{n-k}$\n",
    "\n",
    "    - 로그 우도 함수는 다음과 같다.\n",
    "\n",
    "        - $\\ell(p) = \\log L(p) = \\log \\binom{n}{k} + k \\log p + (n-k) \\log (1-p)$\n",
    "\n",
    "    - 이 로그 우도 함수를 최대화하면 $p$에 대한 MLE 추정값은 다음과 같이 구할 수 있다.\n",
    "\n",
    "        - $\\hat{p} = \\frac{k}{n}$\n",
    "\n",
    "    - 예를 들어, 동전을 10번 던져서 앞면이 7번 나왔다면, 𝑝는 7/10=0.7로 추정된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE로 추정된 p: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DA\\AppData\\Local\\Temp\\ipykernel_13108\\3780334472.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  return -((x * np.log(p)) + ((n - x) * np.log(1 - p)))\n"
     ]
    }
   ],
   "source": [
    "# 환경 설정\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# 데이터 설정\n",
    "n = 10\n",
    "x = 7\n",
    "\n",
    "# 우도 함수 정의\n",
    "def likelihood(p):\n",
    "    return -((x * np.log(p)) + ((n - x) * np.log(1 - p)))\n",
    "\n",
    "# 최적화\n",
    "result = minimize(likelihood, x0=0.5, bounds=[(0, 1)])\n",
    "p_mle = result.x[0]\n",
    "print(f'MLE로 추정된 p: {p_mle:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 정규 분포에서의 MLE 적용\n",
    "\n",
    "    - 정규 분포의 경우, 매개변수 평균 $\\mu$와 표준편차 $\\sigma$(분산 𝜎2의 제곱근)를 추정한다.\n",
    "\n",
    "        - 이 경우, 평균의 MLE는 표본 평균, 분산의 MLE는 표본 분산으로 계산된다.\n",
    "    \n",
    "    - 우도 함수는 다음과 같이 정의된다.\n",
    "\n",
    "        - $L(\\mu, \\sigma) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$\n",
    "\n",
    "    - 로그 우도 함수를 최대화하여 추정값을 구하면, 다음과 같은 결과를 얻는다.\n",
    "\n",
    "        - $\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_i, \\quad \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE로 추정된 평균: 3.3500, 분산: 0.2958\n"
     ]
    }
   ],
   "source": [
    "data = np.array([2.5, 2.9, 3.2, 3.6, 3.8, 4.1])\n",
    "mu_mle = np.mean(data)\n",
    "sigma2_mle = np.var(data)\n",
    "print(f'MLE로 추정된 평균: {mu_mle:.4f}, 분산: {sigma2_mle:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 포아송 분포에서의 MLE 적용\n",
    "\n",
    "    - 포아송 분포의 경우, 사건 발생 횟수 $k$가 주어졌을 때, 매개변수 $\\lambda$를 추정한다.\n",
    "\n",
    "        - 𝜆의 MLE는 관측된 데이터의 평균이다.\n",
    "    \n",
    "    - 우도 함수는 다음과 같다.\n",
    "\n",
    "        - $L(\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$\n",
    "\n",
    "    - 로그 우도 함수는 다음과 같다.\n",
    "\n",
    "        - $\\ell(\\lambda) = k \\log \\lambda - \\lambda - \\log(k!)$\n",
    "\n",
    "    - MLE를 통해 추정값은 다음과 같이 구할 수 있다.\n",
    "\n",
    "        - $\\hat{\\lambda} = \\frac{1}{n} \\sum_{i=1}^{n} k_i$\n",
    "\n",
    "4) 기타 분포에서의 MLE 적용\n",
    "\n",
    "    - 기타 분포에서도 MLE를 적용할 수 있으며, 대표적으로 지수 분포와 다항 분포가 있다.\n",
    "    \n",
    "    - 예를 들어, 지수 분포의 경우, 우도 함수는 다음과 같이 정의된다.\n",
    "\n",
    "        - $L(\\lambda) = \\prod_{i=1}^{n} \\lambda e^{-\\lambda x_i}$\n",
    "\n",
    "    - 로그 우도 함수는 다음과 같다.\n",
    "\n",
    "        - $\\ell(\\lambda) = n \\log \\lambda - \\lambda \\sum_{i=1}^{n} x_i$\n",
    "\n",
    "    - MLE를 통해 추정값은 다음과 같이 구할 수 있다.\n",
    "\n",
    "        - $\\hat{\\lambda} = \\frac{1}{\\bar{x}} \\quad (\\bar{x}는 데이터의 평균)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 최대우도추정법의 성질\n",
    "\n",
    "1) 점근적 일치성(Asymptotic Consistency)\n",
    "\n",
    "    - 정의: MLE는 표본 크기가 충분히 커질수록, 실제 매개변수에 수렴하는 성질을 가진다.\n",
    "    \n",
    "        - 즉, $n \\to \\infty$일 때 $\\hat{\\theta} \\to \\theta$가 성립한다.\n",
    "\n",
    "    - 수학적 표현: $\\text{if } n \\to \\infty, \\text{ then } \\hat{\\theta} \\xrightarrow{P} \\theta$\n",
    "\n",
    "        - 여기서 $ \\xrightarrow{P} $는 확률 수렴을 나타내며, 이는 MLE가 점근적으로 일치함을 의미한다.\n",
    "\n",
    "    - 의의: 이 성질은 MLE가 충분히 큰 표본을 사용할 경우 신뢰할 수 있는 추정치를 제공한다는 것을 보장한다.\n",
    "    \n",
    "        - 즉, 실제 모수를 잘 추정할 수 있다는 것이다.\n",
    "\n",
    "2) 효율성(Efficiency)\n",
    "\n",
    "    - 정의: 효율성은 MLE가 비편향 추정량 중에서 분산이 가장 작은 특성을 가진다는 것을 의미한다.\n",
    "    \n",
    "        - 즉, MLE는 비편향 추정량 중에서 가장 \"최적\"인 추정량으로 간주된다.\n",
    "\n",
    "    - 최소 분산 불편 추정치(MVUE)\n",
    "    \n",
    "        - MLE는 일반적으로 최소 분산을 가지는 비편향 추정량(MVUE)이다.\n",
    "        \n",
    "        - 이는 MLE가 제공하는 추정치의 분산이 다른 비편향 추정량의 분산보다 작다는 것을 의미한다.\n",
    "\n",
    "    - 의의: 효율성은 MLE가 주어진 데이터로부터 얻을 수 있는 정보의 최대화를 반영한다.\n",
    "    \n",
    "        - 즉, MLE는 표본에서 추출할 수 있는 정보를 최대한 활용하여 가장 정확한 추정치를 제공하게 된다.\n",
    "\n",
    "3) 불편성(Unbiasedness, 不偏性)\n",
    "\n",
    "    1) 불편성 정의: 추정치의 기대값이 실제 모수와 일치하는가를 나타낸다.\n",
    "    \n",
    "        - 즉, 여러 번의 표본 추출을 통해 계산된 추정치들의 평균이 참된 모수와 같은 경우, 그 추정 방법은 불편성을 가진다.\n",
    "\n",
    "        - 불편성이 작다는 것은 추정치가 실제 모수에 가깝다는 의미이다. 즉, 추정량이 참된 모수를 잘 반영한다는 것이다.\n",
    "\n",
    "    2) MLE와 불편성의 관계\n",
    "\n",
    "        - 일반적인 경우 (특정 조건이 없을 때)\n",
    "\n",
    "            - MLE는 일반적으로 불편성이 보장되지 않는다.\n",
    "            \n",
    "            - 즉, 작은 표본 크기나 특정 상황에서는 MLE 추정치가 참 모수와 일치하지 않는 편향된 결과를 나타낼 수 있다.\n",
    "\n",
    "        - 특정 조건 (큰 표본 크기 등)\n",
    "\n",
    "            - MLE는 특정 조건에서 불편성을 가질 수 있다.\n",
    "            \n",
    "            - 예를 들어, 표본 크기가 매우 크면 MLE는 점근적으로 불편성을 가지게 되어, 추정치의 기대값이 실제 모수와 일치하게 된다.\n",
    "            \n",
    "            - 이를 점근적 불편성이라고 한다.\n",
    "\n",
    "        - 점근적 불편성: MLE는 일반적으로 표본 크기가 증가함에 따라 불편성이 감소하고, 무한히 큰 표본에서는 실제 모수와 일치하게 된다.\n",
    "        \n",
    "            - 이 점근적 불편성을 '점근적 일치성'이라고 한다.\n",
    "\n",
    "        - 수치적 보정: MLE의 편향을 줄이기 위해 추가적인 보정 기법을 사용할 수 있다.\n",
    "        \n",
    "            - 그러나 이는 반드시 필요한 것은 아니며, 보정은 상황에 따라 적용된다.\n",
    "\n",
    "        - 결론: 따라서 MLE는 반드시 불편성을 가지지는 않지만, 큰 표본에서는 점근적으로 불편성이 줄어든다.\n",
    "\n",
    "    3) 불편성 관련 개념 정리\n",
    "\n",
    "        - 불편성을 가진다: 추정량의 기대값이 실제 모수와 일치한다.\n",
    "\n",
    "        - 불편성을 갖지 않는다: 추정치가 참 모수에서 체계적으로 벗어난다.\n",
    "\n",
    "        - 불편성이 커진다: 추정량이 실제 모수에서 멀어지며, 편향이 증가한다.\n",
    "\n",
    "        - 불편성이 줄어든다: 추정량이 실제 모수에 가까워지며, 편향이 감소한다.\n",
    "\n",
    "        - 불편성을 보장한다: 편향되지 않고 참 모수에 가까운 추정치를 제공하는 특성을 유지하려는 노력이다.\n",
    "\n",
    "        - 불편한 추정치 (Unbiased Estimator)\n",
    "\n",
    "            - 모수(parameter)의 실제 값과 기대값이 일치하는 추정치이다.\n",
    "            \n",
    "            - 즉, 여러 번의 샘플링을 통해 얻은 추정치의 평균이 실제 모수 값과 같다는 의미다.\n",
    "            \n",
    "            - 예를 들어, 표본 평균은 모집단 평균에 대한 불편한 추정치이다.\n",
    "\n",
    "        - 편향된 추정치 (Biased Estimator)\n",
    "\n",
    "            - 모수의 실제 값과 기대값이 일치하지 않는 추정치이다.\n",
    "            \n",
    "            - 즉, 여러 번의 샘플링을 통해 얻은 추정치의 평균이 실제 모수 값과 다르다는 의미다.\n",
    "            \n",
    "            - 예를 들어, 표본의 특정 방법으로 계산된 평균이 모집단 평균보다 항상 높거나 낮은 경우 편향이 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **결론:** 점근적 일치성, 효율성, 불편성은 MLE가 통계적 추정에서 매우 강력하고 신뢰할 수 있는 도구로 자리 잡도록 한다. 점근적 일치성 덕분에 MLE는 큰 표본에서 실제 모수에 수렴하고, 효율성 덕분에 가장 작은 분산을 가진 비편향 추정량으로 기능하며, 불편성 덕분에 평균적으로 정확한 추정치를 제공한다. 이러한 특성들은 MLE를 다양한 통계적 분석과 모델링에 널리 사용되도록 하는 중요한 이유이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. MLE의 장점과 한계\n",
    "\n",
    "1) MLE의 장점: 일반성, 수치적 접근 가능성\n",
    "\n",
    "    - MLE는 매우 일반적이고 다양한 통계 모델에 적용할 수 있다.\n",
    "    \n",
    "    - 또한, 수치적으로도 접근할 수 있어 실용적이다. (수치적 최적화 기법을 통해 추정 가능)\n",
    "\n",
    "    - 데이터의 특성을 반영하는 매개변수를 추정할 수 있다.\n",
    "\n",
    "2) MLE의 한계: 계산 복잡성, 극값 존재 여부\n",
    "\n",
    "    - 복잡한 모델에서는 우도 함수의 최적화를 위한 계산이 매우 복잡해질 수 있다.\n",
    "    \n",
    "    - 또한, 우도 함수의 극값이 존재하지 않거나 다수의 극값이 존재할 수 있다.\n",
    "\n",
    "    - 작은 표본 크기에서는 추정치의 분산이 커질 수 있다.\n",
    "\n",
    "3) 작은 표본 크기에서의 문제점\n",
    "\n",
    "    - 작은 표본 크기에서는 MLE의 성능이 저하될 수 있으며, 특히 불편성이 문제가 될 수 있다.\n",
    "    \n",
    "    - 이럴 경우, 다른 추정 방법(예: 베이즈 추정)을 고려할 필요가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 최대우도추정법의 확장\n",
    "\n",
    "1) 베이즈 추정과의 비교\n",
    "\n",
    "    - 베이즈 추정은 사전 확률과 우도를 결합하여 매개변수를 추정하는 방법이다.\n",
    "    \n",
    "    - MLE는 빈도주의적 접근법으로 사전 정보를 고려하지 않지만, 베이즈 방법은 이를 포함하여 모수를 추정한다.\n",
    "\n",
    "2) 우도비 검정 (Likelihood Ratio Test)\n",
    "\n",
    "    - MLE는 모델 비교에서 자주 사용되며, 우도비 검정을 통해 두 모델의 우도를 비교할 수 있다.\n",
    "    \n",
    "    - 두 모델의 우도를 비교하여, 더 적합한 모델을 선택할 수 있다.\n",
    "\n",
    "3) EM 알고리즘을 통한 MLE 확장\n",
    "\n",
    "    - 데이터가 불완전하거나 잠재 변수(latent variable)가 있는 경우, EM 알고리즘을 사용하여 MLE를 계산할 수 있다.\n",
    "\n",
    "    - EM 알고리즘(Expectation-Maximization Algorithm)\n",
    "    \n",
    "        - 결측값이 있는 경우 MLE를 계산하는 데 유용한 방법으로, 이 알고리즘은 두 단계로 구성된다.\n",
    "\n",
    "            - E-단계 (Expectation Step): 현재의 매개변수를 기반으로 결측값의 기대값을 계산한다.\n",
    "\n",
    "            - M-단계 (Maximization Step): E-단계에서 계산된 기대값을 사용하여 매개변수를 업데이트하여 로그 우도를 최대화한다.\n",
    "\n",
    "        - 이 과정을 반복하여 수렴할 때까지 진행하며, EM 알고리즘은 특히 혼합 모델과 같은 복잡한 모델에서 효과적이다.\n",
    "\n",
    "4) 다변량 MLE 기법 (예: 다변량 정규 분포 등)\n",
    "\n",
    "    - 다변량 MLE는 다변량 데이터의 매개변수를 추정하는 방법이다.\n",
    "    \n",
    "    - 예를 들어, 다변량 정규 분포의 경우, 평균 벡터와 공분산 행렬을 추정한다.\n",
    "    \n",
    "    - 우도 함수는 다음과 같이 정의된다.\n",
    "\n",
    "        - $L(\\mu, \\Sigma) = \\prod_{i=1}^{n} \\frac{1}{(2\\pi)^{k/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x_i - \\mu)^T \\Sigma^{-1} (x_i - \\mu)\\right)$\n",
    "\n",
    "    - 로그 우도 함수는 다음과 같다.\n",
    "\n",
    "        - $\\ell(\\mu, \\Sigma) = -\\frac{n}{2} \\log(2\\pi) - \\frac{n}{2} \\log |\\Sigma| - \\frac{1}{2} \\sum_{i=1}^{n} (x_i - \\mu)^T \\Sigma^{-1} (x_i - \\mu)$\n",
    "\n",
    "    - 이 로그 우도 함수를 최대화하면 $\\mu$와 $\\Sigma$에 대한 MLE를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. MLE의 실무적 적용\n",
    "\n",
    "1) MLE를 활용한 회귀 분석\n",
    "\n",
    "    - 회귀 분석에서 MLE는 회귀 계수를 추정하는 데 사용된다.\n",
    "    \n",
    "    - 예를 들어, 선형 회귀 모델에서는 $\\beta$ 매개변수를 추정하기 위해 MLE를 사용할 수 있다.\n",
    "    \n",
    "    - 이 경우, 오차가 정규 분포를 따른다고 가정하고 로그 우도 함수를 최대화하여 회귀 계수를 추정한다.\n",
    "\n",
    "\n",
    "2) 머신러닝 모델의 파라미터 추정에서의 활용\n",
    "\n",
    "    - MLE는 머신러닝 모델의 학습 과정에서 손실 함수로 자주 사용된다.\n",
    "\n",
    "    - ML 알고리즘(예: 로지스틱 회귀, 서포트 벡터 머신 등)에서 MLE를 사용하여 모델의 파라미터를 추정한다.\n",
    "    \n",
    "    - 모델의 예측이 주어진 데이터의 우도를 최대화하는 방향으로 파라미터를 조정한다.\n",
    "\n",
    "3) 실제 사례 연구 및 응용 분야\n",
    "\n",
    "    - MLE는 생물통계학, 경제학, 공학, 금융 데이터 분석 등 다양한 분야에서 활용된다.\n",
    "    \n",
    "    - 예를 들어, 생물통계학에서는 질병의 발생 확률을 추정하기 위해 MLE를 사용하고, 경제학에서는 소비자 행동 모델링에 적용된다.\n",
    "\n",
    "4) MLE의 소프트웨어 구현 (Python을 통한 구현)\n",
    "\n",
    "    - Python에서는 scipy와 statsmodels와 같은 라이브러리를 사용하여 MLE를 쉽게 구현할 수 있다.\n",
    "    \n",
    "    - 다음은 MLE를 활용한 회귀 분석의 예시이다.\n",
    "\n",
    "        - statsmodels 라이브러리를 사용하여 선형 회귀 모델을 적합시키고, MLE를 통해 매개변수를 추정한다.\n",
    "        \n",
    "        - results.summary()는 회귀 분석 결과를 요약하여 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.747\n",
      "Model:                            OLS   Adj. R-squared:                  0.744\n",
      "Method:                 Least Squares   F-statistic:                     289.3\n",
      "Date:                Mon, 19 Aug 2024   Prob (F-statistic):           5.29e-31\n",
      "Time:                        02:24:11   Log-Likelihood:                -72.200\n",
      "No. Observations:                 100   AIC:                             148.4\n",
      "Df Residuals:                      98   BIC:                             153.6\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.1111      0.097     21.843      0.000       1.919       2.303\n",
      "x1             2.9685      0.175     17.009      0.000       2.622       3.315\n",
      "==============================================================================\n",
      "Omnibus:                       11.746   Durbin-Watson:                   2.083\n",
      "Prob(Omnibus):                  0.003   Jarque-Bera (JB):                4.097\n",
      "Skew:                           0.138   Prob(JB):                        0.129\n",
      "Kurtosis:                       2.047   Cond. No.                         4.30\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# 환경 설정\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 데이터 생성\n",
    "np.random.seed(0)\n",
    "n = 100\n",
    "X = np.random.rand(n, 1)\n",
    "y = 2 + 3 * X.squeeze() + np.random.normal(0, 0.5, n)\n",
    "\n",
    "# MLE를 이용한 선형 회귀 모델 적합\n",
    "X = sm.add_constant(X)  # 상수항 추가\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# 결과 출력\n",
    "print(results.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
